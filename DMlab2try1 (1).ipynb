{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdXf3LtU480Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz3E8MQXj2q_"
      },
      "outputs": [],
      "source": [
        "df_identity = pd.read_csv('/content/data_identification.csv')\n",
        "df_emotion = pd.read_csv('/content/emotion.csv')\n",
        "df_tweet = pd.read_json('/content/tweets_DM.json', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHkyTI_sVkK",
        "outputId": "6d019ab8-0d6b-4478-fb16-6d07bda7c4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             _source\n",
            "0  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...\n",
            "1  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...\n",
            "2  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...\n",
            "3  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...\n",
            "4  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...\n"
          ]
        }
      ],
      "source": [
        "df_tweet_source = df_tweet[['_source']]\n",
        "print(df_tweet_source.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7kSBUrYsjtF",
        "outputId": "30edd71c-c536-4f39-90db-60b7c9f7abb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       _source_split\n",
            "0  [{'tweet': {'hashtags': ['Snapchat'],  'tweet_...\n",
            "1  [{'tweet': {'hashtags': ['freepress',  'TrumpL...\n",
            "2  [{'tweet': {'hashtags': ['bibleverse'],  'twee...\n",
            "3  [{'tweet': {'hashtags': [],  'tweet_id': '0x1c...\n",
            "4  [{'tweet': {'hashtags': [],  'tweet_id': '0x2d...\n"
          ]
        }
      ],
      "source": [
        "df_tweet['_source_str'] = df_tweet['_source'].apply(str)\n",
        "df_tweet['_source_split'] = df_tweet['_source_str'].apply(lambda x: x.split(','))\n",
        "print(df_tweet[['_source_split']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3eo9AolsmqZ",
        "outputId": "9aac26e1-32a0-4800-b930-da3a00ccc9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      _source_cleaned\n",
            "0    'tweet_id': '0x376b20', 'text': 'People who p...\n",
            "1    'TrumpLegacy', 'CNN'], 'tweet_id': '0x2d5350'...\n",
            "2    'tweet_id': '0x28b412', 'text': 'Confident of...\n",
            "3    'tweet_id': '0x1cd5b0', 'text': 'Now ISSA is ...\n",
            "4    'tweet_id': '0x2de201', 'text': '\"Trust is no...\n",
            "5    'LaughOutLoud'], 'tweet_id': '0x1d755c', 'tex...\n",
            "6    'tweet_id': '0x2c91a8', 'text': 'Still waitin...\n",
            "7    'tweet_id': '0x368e95', 'text': 'Love knows n...\n",
            "8    'tweet_id': '0x249c0c', 'text': '@DStvNgCare ...\n",
            "9    'money', 'possessions'], 'tweet_id': '0x21844...\n",
            "10   'gender', 'diversity'], 'tweet_id': '0x359db9...\n",
            "11   'tweet_id': '0x23b037', 'text': \"I love suffe...\n",
            "12   'tweet_id': '0x1fde89', 'text': 'Can someone ...\n",
            "13   'ecology'], 'tweet_id': '0x37a0a9', 'text': '...\n",
            "14   'tweet_id': '0x269112', 'text': \"My brother d...\n",
            "15   'tweet_id': '0x360665', 'text': 'On a scale o...\n",
            "16   'evatech', 'bendingcomposite', 'inovarsandton...\n",
            "17   'tweet_id': '0x25be54', 'text': 'Vomi post bi...\n",
            "18   'tweet_id': '0x33832f', 'text': 'One of my dr...\n",
            "19   'tweet_id': '0x2e0b03', 'text': 'Thankful for...\n"
          ]
        }
      ],
      "source": [
        "df_tweet['_source_cleaned'] = df_tweet['_source_str'].apply(lambda x: x.split(',', 1)[-1] if ',' in x else x)\n",
        "print(df_tweet[['_source_cleaned']].head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvtjD1v5s2ko",
        "outputId": "d52b7737-6e79-493b-ad24-8a8697d53509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     _source_cleaned\n",
            "0  'tweet_id': '0x376b20', 'text': 'People who po...\n",
            "1  'tweet_id': '0x2d5350', 'text': '@brianklaas A...\n",
            "2  'tweet_id': '0x28b412', 'text': 'Confident of ...\n",
            "3  'tweet_id': '0x1cd5b0', 'text': 'Now ISSA is s...\n",
            "4  'tweet_id': '0x2de201', 'text': '\"Trust is not...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_tweet_info(source_str):\n",
        "    # Regular expression to find 'tweet_id' and 'text' and their values\n",
        "    tweet_id_match = re.search(r\"'tweet_id':\\s*'([^']*)'\", source_str)\n",
        "    text_match = re.search(r\"'text':\\s*'([^']*)'\", source_str)\n",
        "\n",
        "    # Extract 'tweet_id' and 'text' if found\n",
        "    tweet_id = tweet_id_match.group(0) if tweet_id_match else None\n",
        "    text = text_match.group(0) if text_match else None\n",
        "\n",
        "    # Combine the results (only if both are found)\n",
        "    return ', '.join(filter(None, [tweet_id, text]))\n",
        "\n",
        "# Apply the function to the '_source' column\n",
        "df_tweet['_source_cleaned'] = df_tweet['_source_str'].apply(extract_tweet_info)\n",
        "\n",
        "# Display the first few rows with the cleaned data\n",
        "print(df_tweet[['_source_cleaned']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJO8gLO1s5V_",
        "outputId": "1dda39e1-890f-4b04-8329-5a85dee9f89f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   tweet_id                                               text\n",
            "0  0x376b20  People who post \"add me on #Snapchat\" must be ...\n",
            "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n",
            "2  0x28b412  Confident of your obedience, I write to you, k...\n",
            "3  0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>\n",
            "4  0x2de201  \"Trust is not the same as faith. A friend is s...\n"
          ]
        }
      ],
      "source": [
        "# Extract tweet_id and text into separate columns using regular expressions\n",
        "df_tweet['tweet_id'] = df_tweet['_source_cleaned'].apply(lambda x: re.search(r\"'tweet_id':\\s*'([^']*)'\", x).group(1) if re.search(r\"'tweet_id':\\s*'([^']*)'\", x) else None)\n",
        "df_tweet['text'] = df_tweet['_source_cleaned'].apply(lambda x: re.search(r\"'text':\\s*'([^']*)'\", x).group(1) if re.search(r\"'text':\\s*'([^']*)'\", x) else None)\n",
        "df_tweet = df_tweet.drop(columns=['_source'])\n",
        "\n",
        "# Display the first few rows to verify the result\n",
        "print(df_tweet[['tweet_id', 'text']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHCP8LHfj6Zb"
      },
      "outputs": [],
      "source": [
        "# merge the datasets\n",
        "df_identity['tweet_id'] = df_identity['tweet_id'].str.strip()\n",
        "df_emotion['tweet_id'] = df_emotion['tweet_id'].str.strip()\n",
        "df_tweet['tweet_id'] = df_tweet['tweet_id'].str.strip()\n",
        "df_merged = pd.merge(df_identity, df_emotion, on='tweet_id', how='outer')\n",
        "df_merged = pd.merge(df_merged, df_tweet, on='tweet_id', how='outer')\n",
        "\n",
        "df_train = df_merged[df_merged['identification'] == 'train']\n",
        "df_test = df_merged[df_merged['identification'] == 'test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcM6SaWwj8-h"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['_crawldate', '_index', '_type', '_source_str', '_source_split', '_source_cleaned']\n",
        "df_train = df_train.drop(columns=columns_to_drop, axis=1)\n",
        "df_test = df_test.drop(columns=columns_to_drop, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdkEKW92j_wq"
      },
      "outputs": [],
      "source": [
        "# clean the 'NaN'\n",
        "df_train['text'] = df_train['text'].fillna(\"\")\n",
        "df_test['text'] = df_test['text'].fillna(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDkWHaT3kGbe",
        "outputId": "2347eb79-ced9-42c0-847a-7423220f0637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1455563, 500)\n",
            "y_train shape: (1455563, 8)\n"
          ]
        }
      ],
      "source": [
        "BOW_vectorizer = CountVectorizer(max_features=500)  # use 500 features\n",
        "BOW_vectorizer.fit(df_train['text'])\n",
        "# feature extraction\n",
        "X_train = BOW_vectorizer.transform(df_train['text'])\n",
        "X_test = BOW_vectorizer.transform(df_test['text'])\n",
        "\n",
        "y_train = df_train['emotion']\n",
        "\n",
        "# label coding\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train_encoded)\n",
        "\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCpgkR0IYmPd",
        "outputId": "748dbbfc-7ec2-4d9f-a2ee-a3bd6abce62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test shape: (411972, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# change the array to dense arrray, this step is added after an error occurred while training the model\n",
        "X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
        "\n",
        "print(\"Any NaN in X_test:\", np.any(np.isnan(X_test_dense)))\n",
        "print(\"Any Inf in X_test:\", np.any(np.isinf(X_test_dense)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWNe5gaKZf2a",
        "outputId": "c76d0c8d-3d44-48f8-e462-e10989860f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any NaN in X_test: False\n",
            "Any Inf in X_test: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLaJMOKWkNYz",
        "outputId": "e10f2c03-3023-458b-b6b3-1def8ef658c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # X_train 的特徵數\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='softmax'))  # y_train 的列數對應於分類數\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MabMuEmikRyT",
        "outputId": "2e33f392-796a-45eb-8203-b4fa9fd5e914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 5ms/step - accuracy: 0.4410 - loss: 1.5313 - val_accuracy: 0.4606 - val_loss: 1.4771\n",
            "Epoch 2/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 5ms/step - accuracy: 0.4647 - loss: 1.4628 - val_accuracy: 0.4629 - val_loss: 1.4674\n",
            "Epoch 3/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 5ms/step - accuracy: 0.4694 - loss: 1.4476 - val_accuracy: 0.4636 - val_loss: 1.4655\n",
            "Epoch 4/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 5ms/step - accuracy: 0.4739 - loss: 1.4375 - val_accuracy: 0.4646 - val_loss: 1.4632\n",
            "Epoch 5/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 5ms/step - accuracy: 0.4756 - loss: 1.4314 - val_accuracy: 0.4647 - val_loss: 1.4669\n",
            "Epoch 6/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 5ms/step - accuracy: 0.4782 - loss: 1.4257 - val_accuracy: 0.4652 - val_loss: 1.4654\n",
            "Epoch 7/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 5ms/step - accuracy: 0.4797 - loss: 1.4224 - val_accuracy: 0.4649 - val_loss: 1.4677\n",
            "Epoch 8/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 5ms/step - accuracy: 0.4807 - loss: 1.4202 - val_accuracy: 0.4647 - val_loss: 1.4694\n",
            "Epoch 9/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - accuracy: 0.4816 - loss: 1.4167 - val_accuracy: 0.4647 - val_loss: 1.4681\n",
            "Epoch 10/10\n",
            "\u001b[1m36390/36390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 5ms/step - accuracy: 0.4827 - loss: 1.4140 - val_accuracy: 0.4634 - val_loss: 1.4735\n",
            "模型已保存至: path_to_save_model/my_model.keras\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "import os\n",
        "\n",
        "# path to save the model\n",
        "save_dir = \"path_to_save_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)  # 如果目錄不存在則創建\n",
        "model_filepath = os.path.join(save_dir, \"my_model.keras\")\n",
        "\n",
        "model.save(model_filepath)\n",
        "print(f\"模型已保存至: {model_filepath}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    y_pred = model.predict(X_test_dense)\n",
        "except Exception as e:\n",
        "    print(\"Error during prediction:\", e)\n",
        "    print(\"Ignoring problematic rows.\")\n",
        "    # exclude the data that has error, and only use the remaining datas\n",
        "    valid_indices = np.arange(X_test_dense.shape[0] - 11)\n",
        "    y_pred = model.predict(X_test_dense[valid_indices])\n",
        "y_pred_classes = label_encoder.inverse_transform(y_pred.argmax(axis=1))  # 解碼預測標籤\n",
        "\n",
        "# prediction output\n",
        "df_test['predicted_emotion'] = y_pred_classes\n",
        "df_test.to_csv('test_predictions.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQPPpa9-nb6B",
        "outputId": "063d50fd-e47b-40bd-cb28-54a81b75ce05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12875/12875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "TUYVkexrR3ip",
        "outputId": "5f824e6e-914d-4e1b-b820-d062e1492792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m64,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m520\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m218,714\u001b[0m (854.36 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">218,714</span> (854.36 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m72,904\u001b[0m (284.78 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,904</span> (284.78 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m145,810\u001b[0m (569.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">145,810</span> (569.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test.head )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73U1riQ_i8nG",
        "outputId": "862f30a6-6123-410a-dc15-19ef78178eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of          tweet_id identification emotion  _score  \\\n",
            "0        0x1c7f0f           test     NaN      62   \n",
            "3        0x1c7f12           test     NaN     756   \n",
            "4        0x1c7f13           test     NaN     213   \n",
            "8        0x1c7f17           test     NaN     603   \n",
            "9        0x1c7f18           test     NaN     609   \n",
            "...           ...            ...     ...     ...   \n",
            "1867509  0x38fe04           test     NaN     497   \n",
            "1867511  0x38fe06           test     NaN     187   \n",
            "1867524  0x38fe13           test     NaN     139   \n",
            "1867525  0x38fe14           test     NaN     251   \n",
            "1867532  0x38fe1b           test     NaN     611   \n",
            "\n",
            "                                                      text predicted_emotion  \n",
            "0                                                                        joy  \n",
            "3                                                                        joy  \n",
            "4        The only “big plan” you ever had in your life,...      anticipation  \n",
            "8        Looking back on situations old & new, recent o...               joy  \n",
            "9        @jasoninthehouse Why do you insist on talking ...           disgust  \n",
            "...                                                    ...               ...  \n",
            "1867509  \"The Grand Bargain\" The Great American Betraya...           sadness  \n",
            "1867511                                                                  joy  \n",
            "1867524                                                                  joy  \n",
            "1867525  I think @kostakoufos might be the worst player...               joy  \n",
            "1867532                                                                  joy  \n",
            "\n",
            "[411972 rows x 6 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# only keep tweet id and emotion\n",
        "df_test_final = df_test[['tweet_id', 'predicted_emotion']]\n",
        "\n",
        "# save to csv\n",
        "df_test_final.to_csv('test_predictions.csv', index=False)\n",
        "\n",
        "print(df_test_final.head())\n",
        "# change the name of head\n",
        "df_test_final.columns = ['id', 'emotion']\n",
        "\n",
        "df_test_final.to_csv('test_predictions.csv', index=False)\n",
        "\n",
        "print(df_test_final.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOt5gBbBjmgy",
        "outputId": "4a3a3345-4675-45f6-8657-745e7ae5e886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   tweet_id predicted_emotion\n",
            "0  0x1c7f0f               joy\n",
            "3  0x1c7f12               joy\n",
            "4  0x1c7f13      anticipation\n",
            "8  0x1c7f17               joy\n",
            "9  0x1c7f18           disgust\n",
            "         id       emotion\n",
            "0  0x1c7f0f           joy\n",
            "3  0x1c7f12           joy\n",
            "4  0x1c7f13  anticipation\n",
            "8  0x1c7f17           joy\n",
            "9  0x1c7f18       disgust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aaVON5MkYgQ"
      },
      "outputs": [],
      "source": [
        "# plot accuracy curve\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot loss curve\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}